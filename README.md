# Divide_and_Conquer
Divide and Conquer – Classification Using Decision Trees and Rules 
Recorded EEG data from 3 subjects are given. Each subject has 4 datasets, representing EEG data collected in “baseline eye closed”, “baseline eye open”, “meditation”, and “watching video” states, respectively. Use the decision tree, Naïve Bayes, and kNN classification methods to train and test the model, and report the accuracy of the classification by comparing the values of the target feature of the test dataset to the predicted values.
In this lab, in addition to the comparison of performance of different machine learning algorithms, we want to experiment on the effect of mixing EEG data from multiple subjects in training and testing to the model performance. We will test the model performance on data from each individual subject, combined data from two subjects, and combined data from all the three subjects, and use scatter plot to examine the trend of the model performance when the number of subjects increases.
The datasets are tidy, containing 5 columns of data, with the first column the timestamp, and other four columns recorded data from four channels. However, the datasets do not have a header line, so column names must be assigned. We will discretize the data as in the previous lab. Firstly, we compute one approximate entropy value for all the eeg data with the same time stamp, as we did before (see the code that computes entropy below). Secondly, the entropy values should be normalized to the range [0, 1] using the min-max method. Then, we discretize the entropy values by multiplying the entropy value by 10 and then rounding the amplified value into the nearest integer using the round() function. This will discretize entropy values into integers in the range [0, 10].
After that, add a target feature “state” with four possible values, viz., “eye-closed”, “eye-open”, “meditate”, and “video”, split the discretized entropy data into train dataset and test dataset, and train and test the kNN, the Naïve Bayes, and the C5.0 decision trees models on dataset from one individual subject.
Repeat the above process with combined data from 2 and then 3 subjects. Plot the trends of model performance.
Perform the following activities in R and submit your codes and results in Blackboard.
1.	For each subject, load data in 4 CSV files into 4 separate data frames in R. Note that data from each subject are save in a separate folder. Datasets do not have headers. Add column names to the 5 columns, which should be “timestamp”, “ch1”, “ch2”, “ch3”, “ch4”. These are the names of variables in the dataset.
2.	For all the data in the 5 columns, convert them to numeric if their type is character. Also, truncate the timestamp values if they are floating point numbers. This can be done using the trunc() function.
3.	For all the rows with the same timestamp, compute an approximate entropy value using the code given below (customization to the code may be needed). Save the computed entropy values in four new datasets.
4.	Add a target feature named “state” to each new dataset with value “eye-closed”, “eye-open”, “meditate”, or “video”, according to the source of the data.
5.	Randomly sample 75% of each dataset and save them in a training dataset, and save the remaining 25% of each dataset in a testing dataset.
6.	Train and test the kNN model using the training and testing datasets. Set K = the square root of the size of the training dataset.
7.	Train the Naïve Bayes model using the training dataset. Set laplace = 1.
8.	Use the Naïve Bayes model to predict the target feature of the testing dataset.
9.	Train the C5.0 decision tree model using the training dataset. Set trials = 10.
10.	Test the model using the test dataset. Set type = “class”.
11.	Use CrossTable() function to compare the predicted state values to the saved true values for the kNN model, the Naïve Bayes model, and the C5.0 decision tree model, respectively, and analyze the results. Compute the overall misclassification rate for each model. Overall misclassification rate is the ratio of the total number of misclassified samples to the total number of all the samples.
12.	Repeat steps 1-11 for each subject.
13.	For each model, viz., kNN, Naïve Bayes, and C5.0, compute the average misclassification rates across all 3 subjects.
14.	Combine the discretized data from 2 out of 3 subjects and repeat steps 1-11 (or steps 6-11, if you choose to combine the sampled data). Repeat this for each combination of 2 out 3 subjects. There are 3 combinations.
15.	For each model, viz., kNN, Naïve Bayes, and C5.0, compute the average misclassification rates across all 3 combinations of 2 out of 3 subjects.
16.	Combine the discretized data from all the 3 subjects and repeat steps 1-11 (or steps 6-11, if you choose to combine the sampled data).
17.	For each model, viz., kNN, Naïve Bayes, and C5.0, compute the misclassification rates from the combined data from all 3 subjects.
18.	Create a scatterplot (using geom_point()) to show the misclassification rates r (the y-axis) vs. the number of subjects n (x-axis) for each of the 3 algorithms, viz., kNN, Naïve Bayes, and C5.0. Display all the individual misclassification rates with the same n. For example, there are 3 r’s when n = 1, 3 r’s when n = 2, and 1 r when the n = 3. Use different colors to distinguish the models.
19.	On the same scatterplot, using geom_smooth() to create a curve that displays the averaged misclassification rates with varying n’s for each of the 3 models. Use the same colors you used in step 18 for the models.
